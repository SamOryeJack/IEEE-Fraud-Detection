# dsci_598_capstone
This repository will be used for Maryville DSCI 598 Capstone Project - Kaggle IEEE Fraud Detection 


# Project Description
In this competition, you will build a classification model to predict whether or not an online financial transaction is fraudulent. This is a binary classification problem. Some important characteristics of this competition are mentioned below.

The target variable, isFraud, is integer-encoded so that a label value of 0 represents a non-fraudulent transaction and a label value of 1 represents a fraudulent transaction.

The dataset contains 432 features, 49 of which are categorical and 383 of which are numerical. A list of the categorical features is provided on the data pageLinks to an external site. of the competition.
Some of the categorical features contain very many levels. For example, one of the categorical variables contains over 13,000 different values.
Some of the feature columns contain missing values.
The columns in the dataset are split across two files which must be merged.
The dataset contains over 590,000 training observations and 510,000 test observations.
Submissions in this competition is scored using the Area Under the Curve (AUC) metric.
 

# Challenges
The DataFrames encountered in this project will be quite large. For example, the merged but unprocessed training set will take up about 2.7 GB of memory. Kaggle virtual machines provide 16GB of memory. We will need to be careful about memory management, only loading datasets as they are needed and deleting DataFrames from memory as they are no longer needed.
The large number of levels found in some of the categorical features will present a challenge. If you apply one-hot-encoding to these features, one column will be created for each level in the encoded array. This will cause the size of the dataset to explode, resulting in an array that is unlikely to fit into memory. Even it is does fit, our training algorithms will struggle when presented with a dataset containing tens of thousands of features. We will need to perform some exploratory data analysis (EDA) to determine which levels for each categorical variable are the most valuable for predicting the target variable. We will keep only these levels and will discard the rest.
The size of this dataset will likely cause your cross-validation to require a significant amount of time to run. You can employ techniques mentioned in the page Grid Search Execution Time to address this concern.
 

# Recommendations
Perform some EDA to better understand the dataset and the features it contains. Create a notebook specifically for the sake of performing this analysis.
To postpone the challenges involved with working with categorical features, begin by building models that involve only the numerical features. Consider logistic regression, decision tree, random forest, and gradient boosted tree models. Use cross-validation to perform hyper-parameter tuning.  After identifying the optimal model type and parameters, create a new notebook on Kaggle to submit the predictions generated by this model. 

After submitting a model that uses only numerical features, explore adding categorical features. Use statistical tests to determine which levels of each categorical variable have the most predictive value. Keep only these levels and discard the rest. Again consider logistic regression, decision tree, random forest, and gradient boosted tree models and use cross-validation for hyper-parameter tuning. Submit the predictions generated by the optimal model found.

Explore notebooksLinks to an external site. submitted by other uses and read posts submitted to the discussion boardLinks to an external site. to get new ideas. Attempt to borrow and adapt techniques used by other users to see if you can improve your score. You can use anything you find within other notebooks, as long as you are able to explain the code and the methods that you are using.
